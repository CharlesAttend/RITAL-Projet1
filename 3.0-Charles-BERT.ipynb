{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: This framework generates embeddings for each input sentence\n",
      "Embedding: [-1.37173664e-02 -4.28515933e-02 -1.56286228e-02  1.40537536e-02\n",
      "  3.95537503e-02  1.21796280e-01  2.94333547e-02 -3.17523777e-02\n",
      "  3.54959592e-02 -7.93140158e-02  1.75878201e-02 -4.04369757e-02\n",
      "  4.97259907e-02  2.54912470e-02 -7.18699768e-02  8.14968571e-02\n",
      "  1.47072668e-03  4.79627103e-02 -4.50335629e-02 -9.92174819e-02\n",
      " -2.81769522e-02  6.45045862e-02  4.44670357e-02 -4.76217456e-02\n",
      " -3.52952294e-02  4.38671485e-02 -5.28565794e-02  4.33043257e-04\n",
      "  1.01921476e-01  1.64072420e-02  3.26996706e-02 -3.45986746e-02\n",
      "  1.21339457e-02  7.94871226e-02  4.58339555e-03  1.57778654e-02\n",
      " -9.68205370e-03  2.87626162e-02 -5.05806580e-02 -1.55793857e-02\n",
      " -2.87907068e-02 -9.62280296e-03  3.15556414e-02  2.27349475e-02\n",
      "  8.71449634e-02 -3.85027677e-02 -8.84718746e-02 -8.75497516e-03\n",
      " -2.12343577e-02  2.08924040e-02 -9.02078077e-02 -5.25732227e-02\n",
      " -1.05638755e-02  2.88311057e-02 -1.61454957e-02  6.17838651e-03\n",
      " -1.23234345e-02 -1.07337013e-02  2.83353738e-02 -5.28567545e-02\n",
      " -3.58617976e-02 -5.97989373e-02 -1.09055052e-02  2.91566644e-02\n",
      "  7.97979310e-02 -3.27880582e-04  6.83497032e-03  1.32718617e-02\n",
      " -4.24619615e-02  1.87656358e-02 -9.89234746e-02  2.09050030e-02\n",
      " -8.69605988e-02 -1.50151988e-02 -4.86202464e-02  8.04414749e-02\n",
      " -3.67700611e-03 -6.65044114e-02  1.14556782e-01 -3.04228868e-02\n",
      "  2.96631809e-02 -2.80695092e-02  4.64990400e-02 -2.25513596e-02\n",
      "  8.54223073e-02  3.15446779e-02  7.34542012e-02 -2.21861824e-02\n",
      " -5.29678985e-02  1.27130682e-02 -5.27339503e-02 -1.06188759e-01\n",
      "  7.04731494e-02  2.76736710e-02 -8.05531293e-02  2.39649452e-02\n",
      " -2.65124887e-02 -2.17331164e-02  4.35275547e-02  4.84711900e-02\n",
      " -2.37067305e-02  2.85768490e-02  1.11846142e-01 -6.34935871e-02\n",
      " -1.58318635e-02 -2.26170011e-02 -1.31028136e-02 -1.62069674e-03\n",
      " -3.60929072e-02 -9.78297219e-02 -4.67729494e-02  1.76272001e-02\n",
      " -3.97492163e-02 -1.76414513e-04  3.39627787e-02 -2.09633857e-02\n",
      "  6.33662241e-03 -2.59411447e-02  8.10410902e-02  6.14393353e-02\n",
      " -5.44601120e-03  6.48276135e-02 -1.16844043e-01  2.36860979e-02\n",
      " -1.32058617e-02 -1.12476431e-01  1.90049224e-02 -1.74659072e-34\n",
      "  5.58949523e-02  1.94244459e-02  4.65438776e-02  5.18646017e-02\n",
      "  3.89390066e-02  3.40540782e-02 -4.32114117e-02  7.90637583e-02\n",
      " -9.79530215e-02 -1.27441231e-02 -2.91870758e-02  1.02052484e-02\n",
      "  1.88116096e-02  1.08942576e-01  6.63465261e-02 -5.35295196e-02\n",
      " -3.29229049e-02  4.69827019e-02  2.28882749e-02  2.74114516e-02\n",
      " -2.91983206e-02  3.12706642e-02 -2.22850759e-02 -1.02282152e-01\n",
      " -2.79116668e-02  1.13793211e-02  9.06308889e-02 -4.75414470e-02\n",
      " -1.00718930e-01 -1.23232082e-02 -7.96928480e-02 -1.44636836e-02\n",
      " -7.76400566e-02 -7.66919414e-03  9.73956380e-03  2.24204753e-02\n",
      "  7.77267963e-02 -3.17156943e-03  2.11538132e-02 -3.30393985e-02\n",
      "  9.55249276e-03 -3.73012237e-02  2.61360388e-02 -9.79083031e-03\n",
      " -6.31505027e-02  5.77435084e-03 -3.80031131e-02  1.29684191e-02\n",
      " -1.82499122e-02 -1.56283341e-02 -1.23360276e-03  5.55579327e-02\n",
      "  1.13119429e-04 -5.61256856e-02  7.40166008e-02  1.84451956e-02\n",
      " -2.66368315e-02  1.31951673e-02  7.50086755e-02 -2.46797465e-02\n",
      " -3.24006416e-02 -1.57674868e-02 -8.03516991e-03 -5.61319944e-03\n",
      "  1.05687780e-02  3.26170330e-03 -3.91989835e-02 -9.38677117e-02\n",
      "  1.14227168e-01  6.57304898e-02 -4.72633280e-02  1.45088052e-02\n",
      " -3.54490578e-02 -3.37761454e-02 -5.15506081e-02 -3.80999851e-03\n",
      " -5.15036322e-02 -5.93429729e-02 -1.69412722e-03  7.42107853e-02\n",
      " -4.20091338e-02 -7.19975084e-02  3.17250043e-02 -1.66303366e-02\n",
      "  3.96985980e-03 -6.52750880e-02  2.77390927e-02 -7.51649812e-02\n",
      "  2.27456298e-02 -3.91368270e-02  1.54316062e-02 -5.54908514e-02\n",
      "  1.23318136e-02 -2.59520561e-02  6.66423589e-02 -6.91259050e-34\n",
      "  3.31628807e-02  8.47928971e-02 -6.65584058e-02  3.33541594e-02\n",
      "  4.71608527e-03  1.35361888e-02 -5.38694374e-02  9.20694098e-02\n",
      " -2.96876524e-02  3.16219293e-02 -2.37497445e-02  1.98771060e-02\n",
      "  1.03446215e-01 -9.06947404e-02  6.30628550e-03  1.42886145e-02\n",
      "  1.19293369e-02  6.43728254e-03  4.20104563e-02  1.25344619e-02\n",
      "  3.93019505e-02  5.35691455e-02 -4.30749506e-02  6.10432513e-02\n",
      " -5.39006360e-05  6.91682771e-02  1.05520496e-02  1.22111570e-02\n",
      " -7.23185465e-02  2.50469539e-02 -5.18370569e-02 -4.36562374e-02\n",
      " -6.71818107e-02  1.34828435e-02 -7.25888908e-02  7.04164151e-03\n",
      "  6.58939257e-02  1.08994432e-02 -2.60010571e-03  5.49968779e-02\n",
      "  5.06966822e-02  3.27948630e-02 -6.68833107e-02  6.45557195e-02\n",
      " -2.52076313e-02 -2.92571876e-02 -1.16696738e-01  3.24064456e-02\n",
      "  5.85858859e-02 -3.51756550e-02 -7.15240017e-02  2.24935990e-02\n",
      " -1.00786716e-01 -4.74544875e-02 -7.61962533e-02 -5.87166697e-02\n",
      "  4.21138406e-02 -7.47213736e-02  1.98467951e-02 -3.36503028e-03\n",
      " -5.29736429e-02  2.74729207e-02  3.45736668e-02 -6.11846857e-02\n",
      "  1.06364764e-01 -9.64119807e-02 -4.55945134e-02  1.51489973e-02\n",
      " -5.13532665e-03 -6.64447621e-02  4.31721285e-02 -1.10405674e-02\n",
      " -9.80251469e-03  7.53783062e-02 -1.49570880e-02 -4.80208434e-02\n",
      "  5.80726415e-02 -2.43896563e-02 -2.23138127e-02 -4.36992794e-02\n",
      "  5.12054190e-02 -3.28625739e-02  1.08763345e-01  6.08926117e-02\n",
      "  3.30791879e-03  5.53819947e-02  8.43200609e-02  1.27087152e-02\n",
      "  3.84465568e-02  6.52325824e-02 -2.94683799e-02  5.08005656e-02\n",
      " -2.09348183e-02  1.46135688e-01  2.25561522e-02 -1.77227744e-08\n",
      " -5.02672568e-02 -2.79213797e-04 -1.00328520e-01  2.42810994e-02\n",
      " -7.54043311e-02 -3.79139669e-02  3.96049693e-02  3.10079884e-02\n",
      " -9.05703194e-03 -6.50411770e-02  4.05453406e-02  4.83390018e-02\n",
      " -4.56962287e-02  4.76002926e-03  2.64365389e-03  9.35614184e-02\n",
      " -4.02598940e-02  3.27401906e-02  1.18298484e-02  5.54344878e-02\n",
      "  1.48052216e-01  7.21189454e-02  2.76988896e-04  1.68651417e-02\n",
      "  8.34879745e-03 -8.76155682e-03 -1.33650051e-02  6.14236854e-02\n",
      "  1.57168061e-02  6.94960654e-02  1.08621689e-02  6.08018562e-02\n",
      " -5.33421114e-02 -3.47924642e-02 -3.36272195e-02  6.93906993e-02\n",
      "  1.22987945e-02 -1.45237371e-01 -2.06972961e-03 -4.61133085e-02\n",
      "  3.72749148e-03 -5.59356716e-03 -1.00659862e-01 -4.45953235e-02\n",
      "  5.40921465e-02  4.98894975e-03  1.49534717e-02 -8.26059356e-02\n",
      "  6.26630560e-02 -5.01909852e-03 -4.81857881e-02 -3.53991166e-02\n",
      "  9.03387833e-03 -2.42337547e-02  5.66267222e-02  2.51529124e-02\n",
      " -1.70709584e-02 -1.24779986e-02  3.19518484e-02  1.38421040e-02\n",
      " -1.55815110e-02  1.00178272e-01  1.23657264e-01 -4.22967076e-02]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2', device='cuda')\n",
    "\n",
    "#Our sentences we like to encode\n",
    "sentences = ['This framework generates embeddings for each input sentence']\n",
    "\n",
    "#Sentences are encoded by calling model.encode()\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "#Print the embeddings\n",
    "for sentence, embedding in zip(sentences, embeddings):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding:\", embedding)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./src')\n",
    "\n",
    "from src.data import load_data_part1, CustomAnalyzer\n",
    "# from src.eval import fit_eval\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_data_part1()\n",
    "X_embeded = model.encode(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((57413, 384), (57413,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_embeded.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, f1_score, roc_auc_score\n",
    "\n",
    "def print_score(y_test, pred, name):\n",
    "    macro_f1 = f1_score(y_test, pred, average=\"macro\")\n",
    "    micro_f1 = f1_score(y_test, pred, average=\"micro\")\n",
    "    macro_auc = roc_auc_score(y_test, pred)\n",
    "    micro_auc = roc_auc_score(y_test, pred, average=\"weighted\")\n",
    "    classif_report = classification_report(y_test, pred, output_dict=True)\n",
    "\n",
    "    reports = f\"\"\"\n",
    "    {name} :\n",
    "    =====\n",
    "    Macro F1-score : {macro_f1}\n",
    "    Micro F1-score : {micro_f1}\n",
    "    Macro ROC-AUC: {macro_auc}\n",
    "    Weighted ROC-AUC: {micro_auc}\n",
    "    Classification report :\n",
    "    {classification_report(y_test, pred)}\n",
    "    =====\n",
    "    \"\"\"\n",
    "    print(reports)\n",
    "\n",
    "    classif_report[\"macro_auc\"] = macro_auc\n",
    "    classif_report[\"micro_auc\"] = micro_auc\n",
    "    return classif_report\n",
    "\n",
    "\n",
    "def fit_eval(X_train, y_train, X_test, y_test, balanced=None):\n",
    "    \"\"\"\n",
    "    Entraine et évalue les algorithmes classiques de classification à partir des \n",
    "    différents sets de données.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train: Sparse Matrix\n",
    "        Important : Une matrice BoW est attendu\n",
    "\n",
    "    X_test: Sparse Matrix\n",
    "        Important : Une matrice BoW est attendu\n",
    "\n",
    "    y_train: list\n",
    "        Label des données de train\n",
    "\n",
    "    y_test: list\n",
    "        Label des données de test\n",
    "\n",
    "    balanced: None ou autre\n",
    "    \"\"\"\n",
    "    # Naïve Bayes\n",
    "    # if balanced is None:\n",
    "    #     nb_clf = MultinomialNB()\n",
    "    # else:\n",
    "    #     balanced = \"balanced\"\n",
    "    #     nb_clf = MultinomialNB(fit_prior=True)\n",
    "    # nb_clf.fit(X_train, y_train)\n",
    "\n",
    "    # Logistic Regression\n",
    "    lr_clf = LogisticRegression(\n",
    "        random_state=0, solver=\"lbfgs\", n_jobs=-1, max_iter=10000, class_weight=balanced\n",
    "    )\n",
    "    lr_clf.fit(X_train, y_train)\n",
    "\n",
    "    # Linear SVM\n",
    "    svm_clf = LinearSVC(random_state=0, tol=1e-5, max_iter=20000, class_weight=balanced)\n",
    "    svm_clf.fit(X_train, y_train)\n",
    "\n",
    "    # pred_nb = nb_clf.predict(X_test)\n",
    "    pred_lr = lr_clf.predict(X_test)\n",
    "    pred_svm = svm_clf.predict(X_test)\n",
    "\n",
    "    # Ridge Classifier ?\n",
    "    results = (\n",
    "        # print_score(y_test, pred_nb, \"Naïve Bayes\"),\n",
    "        print_score(y_test, pred_lr, \"Logistic Regression\"),\n",
    "        print_score(y_test, pred_svm, \"SVM\"),\n",
    "    )\n",
    "    algo_names = [\"Logistic Regression\", \"SVM\"]\n",
    "\n",
    "    return results, algo_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\n",
      "    Logistic Regression :\n",
      "    =====\n",
      "    Macro F1-score : 0.5910171423638484\n",
      "    Micro F1-score : 0.7053086247735822\n",
      "    Macro ROC-AUC: 0.6920615728271845\n",
      "    Weighted ROC-AUC: 0.6920615728271845\n",
      "    Classification report :\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.26      0.67      0.37      1881\n",
      "           1       0.94      0.71      0.81     12473\n",
      "\n",
      "    accuracy                           0.71     14354\n",
      "   macro avg       0.60      0.69      0.59     14354\n",
      "weighted avg       0.85      0.71      0.75     14354\n",
      "\n",
      "    =====\n",
      "    \n",
      "\n",
      "    SVM :\n",
      "    =====\n",
      "    Macro F1-score : 0.5940318387940465\n",
      "    Micro F1-score : 0.7099763132227951\n",
      "    Macro ROC-AUC: 0.692941538411965\n",
      "    Weighted ROC-AUC: 0.692941538411965\n",
      "    Classification report :\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.26      0.67      0.38      1881\n",
      "           1       0.93      0.72      0.81     12473\n",
      "\n",
      "    accuracy                           0.71     14354\n",
      "   macro avg       0.60      0.69      0.59     14354\n",
      "weighted avg       0.85      0.71      0.75     14354\n",
      "\n",
      "    =====\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(({'-1': {'precision': 0.25957011258955986,\n",
       "    'recall': 0.6741095162147793,\n",
       "    'f1-score': 0.37481525273425953,\n",
       "    'support': 1881},\n",
       "   '1': {'precision': 0.9352624353152392,\n",
       "    'recall': 0.7100136294395896,\n",
       "    'f1-score': 0.8072190319934374,\n",
       "    'support': 12473},\n",
       "   'accuracy': 0.7053086247735822,\n",
       "   'macro avg': {'precision': 0.5974162739523996,\n",
       "    'recall': 0.6920615728271844,\n",
       "    'f1-score': 0.5910171423638484,\n",
       "    'support': 14354},\n",
       "   'weighted avg': {'precision': 0.8467172730575407,\n",
       "    'recall': 0.7053086247735822,\n",
       "    'f1-score': 0.7505552791171302,\n",
       "    'support': 14354},\n",
       "   'macro_auc': 0.6920615728271845,\n",
       "   'micro_auc': 0.6920615728271845},\n",
       "  {'-1': {'precision': 0.26239067055393583,\n",
       "    'recall': 0.6698564593301436,\n",
       "    'f1-score': 0.3770761633996708,\n",
       "    'support': 1881},\n",
       "   '1': {'precision': 0.9349874371859297,\n",
       "    'recall': 0.7160266174937866,\n",
       "    'f1-score': 0.8109875141884223,\n",
       "    'support': 12473},\n",
       "   'accuracy': 0.7099763132227951,\n",
       "   'macro avg': {'precision': 0.5986890538699328,\n",
       "    'recall': 0.692941538411965,\n",
       "    'f1-score': 0.5940318387940465,\n",
       "    'support': 14354},\n",
       "   'weighted avg': {'precision': 0.8468479277784626,\n",
       "    'recall': 0.7099763132227951,\n",
       "    'f1-score': 0.7541262036942298,\n",
       "    'support': 14354},\n",
       "   'macro_auc': 0.692941538411965,\n",
       "   'micro_auc': 0.692941538411965}),\n",
       " ['Logistic Regression', 'SVM'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_embeded_train, X_embeded_test, y_train, y_test = train_test_split(X_embeded, y, stratify=y)\n",
    "fit_eval(X_embeded_train, y_train, X_embeded_test, y_test, balanced='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Logistic Regression :\n",
      "    =====\n",
      "    Macro F1-score : 0.73810028158686\n",
      "    Micro F1-score : 0.9030932144350007\n",
      "    Macro ROC-AUC: 0.6968400602291913\n",
      "    Weighted ROC-AUC: 0.6968400602291913\n",
      "    Classification report :\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.73      0.42      0.53      1881\n",
      "           1       0.92      0.98      0.95     12473\n",
      "\n",
      "    accuracy                           0.90     14354\n",
      "   macro avg       0.82      0.70      0.74     14354\n",
      "weighted avg       0.89      0.90      0.89     14354\n",
      "\n",
      "    =====\n",
      "    \n",
      "\n",
      "    SVM :\n",
      "    =====\n",
      "    Macro F1-score : 0.7313071933409601\n",
      "    Micro F1-score : 0.8855371325066184\n",
      "    Macro ROC-AUC: 0.7167602595769542\n",
      "    Weighted ROC-AUC: 0.7167602595769542\n",
      "    Classification report :\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.57      0.49      0.53      1881\n",
      "           1       0.92      0.95      0.93     12473\n",
      "\n",
      "    accuracy                           0.89     14354\n",
      "   macro avg       0.75      0.72      0.73     14354\n",
      "weighted avg       0.88      0.89      0.88     14354\n",
      "\n",
      "    =====\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(({'-1': {'precision': 0.7268518518518519,\n",
       "    'recall': 0.417331206804891,\n",
       "    'f1-score': 0.5302262749071259,\n",
       "    'support': 1881},\n",
       "   '1': {'precision': 0.9174325749585657,\n",
       "    'recall': 0.9763489136534915,\n",
       "    'f1-score': 0.9459742882665941,\n",
       "    'support': 12473},\n",
       "   'accuracy': 0.9030932144350007,\n",
       "   'macro avg': {'precision': 0.8221422134052088,\n",
       "    'recall': 0.6968400602291913,\n",
       "    'f1-score': 0.73810028158686,\n",
       "    'support': 14354},\n",
       "   'weighted avg': {'precision': 0.8924581887133568,\n",
       "    'recall': 0.9030932144350007,\n",
       "    'f1-score': 0.8914931671066972,\n",
       "    'support': 14354},\n",
       "   'macro_auc': 0.6968400602291913,\n",
       "   'micro_auc': 0.6968400602291913},\n",
       "  {'-1': {'precision': 0.574468085106383,\n",
       "    'recall': 0.4880382775119617,\n",
       "    'f1-score': 0.5277378557056627,\n",
       "    'support': 1881},\n",
       "   '1': {'precision': 0.9245061147695203,\n",
       "    'recall': 0.9454822416419466,\n",
       "    'f1-score': 0.9348765309762576,\n",
       "    'support': 12473},\n",
       "   'accuracy': 0.8855371325066184,\n",
       "   'macro avg': {'precision': 0.7494870999379517,\n",
       "    'recall': 0.7167602595769542,\n",
       "    'f1-score': 0.7313071933409601,\n",
       "    'support': 14354},\n",
       "   'weighted avg': {'precision': 0.8786358671872184,\n",
       "    'recall': 0.8855371325066184,\n",
       "    'f1-score': 0.8815236085724685,\n",
       "    'support': 14354},\n",
       "   'macro_auc': 0.7167602595769542,\n",
       "   'micro_auc': 0.7167602595769542}),\n",
       " ['Logistic Regression', 'SVM'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)\n",
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "fit_eval(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Logistic Regression :\n",
      "    =====\n",
      "    Macro F1-score : 0.7282543691980656\n",
      "    Micro F1-score : 0.8528633133621291\n",
      "    Macro ROC-AUC: 0.7753848578746146\n",
      "    Weighted ROC-AUC: 0.7753848578746146\n",
      "    Classification report :\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.46      0.67      0.54      1881\n",
      "           1       0.95      0.88      0.91     12473\n",
      "\n",
      "    accuracy                           0.85     14354\n",
      "   macro avg       0.70      0.78      0.73     14354\n",
      "weighted avg       0.88      0.85      0.86     14354\n",
      "\n",
      "    =====\n",
      "    \n",
      "\n",
      "    SVM :\n",
      "    =====\n",
      "    Macro F1-score : 0.7147905866623758\n",
      "    Micro F1-score : 0.8521666434443361\n",
      "    Macro ROC-AUC: 0.7465420790033533\n",
      "    Weighted ROC-AUC: 0.7465420790033533\n",
      "    Classification report :\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.45      0.60      0.52      1881\n",
      "           1       0.94      0.89      0.91     12473\n",
      "\n",
      "    accuracy                           0.85     14354\n",
      "   macro avg       0.69      0.75      0.71     14354\n",
      "weighted avg       0.87      0.85      0.86     14354\n",
      "\n",
      "    =====\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(({'-1': {'precision': 0.4580457682528151,\n",
       "    'recall': 0.670388091440723,\n",
       "    'f1-score': 0.5442382391022874,\n",
       "    'support': 1881},\n",
       "   '1': {'precision': 0.9465563313507456,\n",
       "    'recall': 0.8803816243085064,\n",
       "    'f1-score': 0.912270499293844,\n",
       "    'support': 12473},\n",
       "   'accuracy': 0.8528633133621291,\n",
       "   'macro avg': {'precision': 0.7023010498017803,\n",
       "    'recall': 0.7753848578746148,\n",
       "    'f1-score': 0.7282543691980656,\n",
       "    'support': 14354},\n",
       "   'weighted avg': {'precision': 0.8825401428884906,\n",
       "    'recall': 0.8528633133621291,\n",
       "    'f1-score': 0.8640422227562713,\n",
       "    'support': 14354},\n",
       "   'macro_auc': 0.7753848578746146,\n",
       "   'micro_auc': 0.7753848578746146},\n",
       "  {'-1': {'precision': 0.4520111509358821,\n",
       "    'recall': 0.6034024455077087,\n",
       "    'f1-score': 0.5168488160291439,\n",
       "    'support': 1881},\n",
       "   '1': {'precision': 0.9370092037490501,\n",
       "    'recall': 0.8896817124989979,\n",
       "    'f1-score': 0.9127323572956079,\n",
       "    'support': 12473},\n",
       "   'accuracy': 0.8521666434443361,\n",
       "   'macro avg': {'precision': 0.6945101773424661,\n",
       "    'recall': 0.7465420790033532,\n",
       "    'f1-score': 0.7147905866623758,\n",
       "    'support': 14354},\n",
       "   'weighted avg': {'precision': 0.8734533073200708,\n",
       "    'recall': 0.8521666434443361,\n",
       "    'f1-score': 0.8608543483000514,\n",
       "    'support': 14354},\n",
       "   'macro_auc': 0.7465420790033533,\n",
       "   'micro_auc': 0.7465420790033533}),\n",
       " ['Logistic Regression', 'SVM'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_eval(X_train, y_train, X_test, y_test, balanced='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
